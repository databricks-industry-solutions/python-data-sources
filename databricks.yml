bundle:
  name: demo_workflow

variables:
  # Project name - replace with your solution accelerator name
  project_name:
    description: "Python Data Sources"
    default: "python-data-sources"
    
  # The "warehouse_id" variable is used to reference the warehouse used by the dashboard.
  warehouse_id:
    lookup:
      # Replace this with the name of your SQL warehouse.
      warehouse: "Shared Unity Catalog Serverless"
      
  # Environment variable used for deployment paths
  environment:
    description: "Deployment environment (dev, staging, prod)"
    default: "dev"

targets:
  dev:
    default: true
    mode: development

# Customize the resources below based on your solution accelerator needs
resources:

  # Example job workflow - customize based on your notebooks
  jobs:
    demo_workflow:
      name: "${var.project_name} - Pytest Workflow"
      tasks:
        - task_key: dbrunner
          spark_python_task:
            python_file: /Workspace/Users/douglas.moore@databricks.com/python-data-sources-x/zipdcm/db_runner.py
          existing_cluster_id: 0519-014005-pr11dvi3
          libraries:
            - pypi:
                package: pyspark==4.0.0.dev1
      git_source:
        git_url: https://github.com/databricks-industry-solutions/python-data-sources.git
        git_provider: gitHub
        git_branch: feat/zipdcm
      tags:
        owner: douglas.moore@databricks.com
        solacc: pixels
      queue:
        enabled: true
      environments:
        - environment_key: zipdcm_pytest_environment
          spec:
            client: "3"
            dependencies:
              - pydicom==3.0.1
              - pytest==8.3.5
      budget_policy_id: d8e5830d-97cb-40b9-bd65-063434295162

# For more options and schema, see: https://docs.databricks.com/aws/en/dev-tools/bundles/settings
